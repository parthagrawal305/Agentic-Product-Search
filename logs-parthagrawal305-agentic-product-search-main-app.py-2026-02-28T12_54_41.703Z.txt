
[ â€‰â€‰â€…â€…UTCâ€…â€…â€‰â€‰ ] Logs for agentic-ecom-search.streamlit.app/
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[12:37:53] ğŸš€ Starting up repository: 'agentic-product-search', branch: 'main', main module: 'app.py'
[12:37:53] ğŸ™ Cloning repository...
[12:37:54] ğŸ™ Cloning into '/mount/src/agentic-product-search'...

[12:37:54] ğŸ™ Cloned repository!
[12:37:54] ğŸ™ Pulling code changes from Github...
[12:37:54] ğŸ“¦ Processing dependencies...

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ uv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Using uv pip install.
Using Python 3.13.12 environment at /home/adminuser/venv
Resolved 112 packages in 1.32s
Prepared 112 packages in 3.01s
Installed 112 packages in 144ms
 + altair==6.0.0
 + annotated-doc==0.0.4
 + annotated-types==0.7.0
 + anyio==4.12.1
 + attrs==25.4.0
 + blinker==1.9.0
 + cachetools==6.2.6
 + certifi==2026.2.25
 + cffi==2.0.0
 + charset-normalizer==3.4.4
 + click==8.3.1
 + cryptography==46.0.5
 + deprecation==2.1.0
 + distro==1.9.0
 + fastembed==0.7.4
 + filelock==3.24.3[2026-02-28 12:37:59.448438] 
 + flatbuffers==25.12.19
 + fsspec==2026.2.0
 + gitdb==4.0.12
 + gitpython==3.1.46
 + groq==0.37.1
 + grpcio==1.78.0
 + h11==0.16.0[2026-02-28 12:37:59.448669] 
 + h2==4.3.0
 + hf-xet==1.3.2
 + hpack==4.1.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==[2026-02-28 12:37:59.449772] 1.5.0
 + hyperframe==6.1.0
 + idna==3.11
 + jinja2==3.1.6
 + jsonpatch==1.33
 + jsonpointer==3.0.0
 + jsonschema==4.26.0
 + jsonschema-specifications==2025.9.1
 + langchain-core==1.2.16
 + langchain-groq==1.1.2
 + langgraph==[2026-02-28 12:37:59.450348] 1.0.10
 + langgraph-checkpoint==4.0.1
 + langgraph-prebuilt==1.0.8
 + langgraph-sdk==0.3.9
 + langsmith==0.7.9
 +[2026-02-28 12:37:59.450550]  loguru==0.7.3
 + markdown-it-py==4.0.0
 + markupsafe==3.0.3
 + mdurl==0.1.2
 + mmh3==5.2.0
 + mpmath==1.3.0
 + multidict==6.7.1
 + narwhals==2.17.0
 + numpy==2.4.2
 + onnxruntime==1.24.2[2026-02-28 12:37:59.451297] 
 + orjson==3.11.7
 + ormsgpack==1.12.2
 + packaging==26.0
 + pandas==2.3.3
 + pillow==11.3.0[2026-02-28 12:37:59.451548] 
 + portalocker==3.2.0
 + postgrest==2.28.0
 + propcache==0.4.1
 + protobuf==6.33.5
 + [2026-02-28 12:37:59.451830] py-rust-stemmers==0.1.5
 + pyarrow==23.0.1
 + pycparser==3.0
 + pydantic[2026-02-28 12:37:59.452071] ==2.12.5
 + pydantic-core==2.41.5
 + pydeck==0.9.1
 + pygments==[2026-02-28 12:37:59.452294] 2.19.2
 + pyiceberg==0.11.0
 + pyjwt==2.11.0
 + pyparsing==3.3.2[2026-02-28 12:37:59.452512] 
 + pyroaring==1.0.3
 + python-dateutil==2.9.0.post0
 + python-dotenv==1.2.1[2026-02-28 12:37:59.452767] 
 + pytz==2025.2
 + pyyaml==6.0.3
 + qdrant-client==[2026-02-28 12:37:59.453021] 1.17.0
 + realtime==2.28.0
 + referencing==0.37.0
 + requests[2026-02-28 12:37:59.453243] ==2.32.5
 + requests-toolbelt==1.0.0
 + rich==14.3.3
 + [2026-02-28 12:37:59.453470] rpds-py==0.30.0
 + shellingham==1.5.4
 + six==1.17.0
 [2026-02-28 12:37:59.453726] + smmap==5.0.2
 + sniffio==1.3.1
 + storage3==2.28.0[2026-02-28 12:37:59.453970] 
 + streamlit==1.54.0
 + strenum==0.4.15
 + strictyaml==1.7.3[2026-02-28 12:37:59.454213] 
 + supabase==2.28.0
 + supabase-auth==2.28.0
 + supabase-functions==2.28.0
 + sympy==1.14.0
 + tenacity==9.1.4
 + tokenizers==0.22.2
 + toml==0.10.2
 + tornado==6.5.4
 + tqdm==4.67.3
 + typer==0.24.1
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.3
 + urllib3==2.6.3
 + uuid-utils==0.14.1
 + watchdog==6.0.0
 + [2026-02-28 12:37:59.454736] websockets==15.0.1
 + xxhash==3.6.0
 + yarl==1.22.0
 + zstandard==0.25.0
Checking if Streamlit is installed
Found Streamlit version 1.54.0 in the environment
Installing rich for an improved exception logging
Using uv pip install.
Using Python 3.13.12 environment at /home/adminuser/venv
Audited 1 package in 5ms

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[12:38:01] ğŸ Python dependencies were installed from /mount/src/agentic-product-search/requirements.txt using uv.
Check if streamlit is installed
Streamlit is already installed
[12:38:03] ğŸ“¦ Processed dependencies!



â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:3 in <module>                        
                                                                                
     1 import streamlit as st                                                   
     2 from langchain_core.messages import HumanMessage, AIMessage              
  â±  3 from src.agent.graph import app_graph                                    
     4 from src.agent.state import AgentState                                   
     5                                                                          
     6 st.set_page_config(page_title="Shopping AI Agent", page_icon="ğŸ›ï¸")       
                                                                                
  /mount/src/agentic-product-search/src/agent/graph.py:3 in <module>            
                                                                                
     1 from langgraph.graph import StateGraph, END                              
     2 from src.agent.state import AgentState                                   
  â±  3 from src.agent.nodes import supervisor_node, search_node                 
     4                                                                          
     5 # Define Conditional Routing Logic                                       
     6 def route_after_supervisor(state: AgentState) -> str:                    
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:18 in <module>           
                                                                                
     15 # Initialize external services securely                                 
     16 SUPABASE_URL = os.environ.get("SUPABASE_URL")                           
     17 SUPABASE_KEY = os.environ.get("SUPABASE_KEY")                           
  â±  18 supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)            
     19                                                                         
     20 qdrant = QdrantClient(path="./qdrant_data")                             
     21 embedding_model = TextEmbedding("BAAI/bge-small-en-v1.5")               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/supabase/_sync/client.py:3  
  80 in create_client                                                           
                                                                                
    377 â”‚   -------                                                             
    378 â”‚   Client                                                              
    379 â”‚   """                                                                 
  â± 380 â”‚   return Client.create(                                               
    381 â”‚   â”‚   supabase_url=supabase_url, supabase_key=supabase_key, options=  
    382 â”‚   )                                                                   
    383                                                                         
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/supabase/_sync/client.py:1  
  09 in create                                                                  
                                                                                
    106 â”‚   â”‚   options: Optional[ClientOptions] = None,                        
    107 â”‚   ) -> "Client":                                                      
    108 â”‚   â”‚   auth_header = options.headers.get("Authorization") if options   
  â± 109 â”‚   â”‚   client = cls(supabase_url, supabase_key, options)               
    110 â”‚   â”‚                                                                   
    111 â”‚   â”‚   if auth_header is None:                                         
    112 â”‚   â”‚   â”‚   try:                                                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/supabase/_sync/client.py:5  
  7 in __init__                                                                 
                                                                                
     54 â”‚   â”‚   """                                                             
     55 â”‚   â”‚                                                                   
     56 â”‚   â”‚   if not supabase_url:                                            
  â±  57 â”‚   â”‚   â”‚   raise SupabaseException("supabase_url is required")         
     58 â”‚   â”‚   if not supabase_key:                                            
     59 â”‚   â”‚   â”‚   raise SupabaseException("supabase_key is required")         
     60                                                                         
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SupabaseException: supabase_url is required
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s][2026-02-28 12:39:45.756910] Fetching 5 files:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:00,  5.82it/s][2026-02-28 12:39:47.302138] Fetching 5 files:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.02it/s][2026-02-28 12:39:47.302425] Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.91it/s]
[12:45:01] ğŸ™ Pulling code changes from Github...
[12:45:02] ğŸ“¦ Processing dependencies...
[12:45:02] ğŸ“¦ Processed dependencies!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :398 in lock                                                                  
                                                                                
    395 â”‚   â”‚   â”‚                                                               
    396 â”‚   â”‚   â”‚   fd = self._get_fd(file_obj)                                 
    397 â”‚   â”‚   â”‚   try:                                                        
  â± 398 â”‚   â”‚   â”‚   â”‚   self.locker(fd, flags)                                  
    399 â”‚   â”‚   â”‚   except OSError as exc_value:                                
    400 â”‚   â”‚   â”‚   â”‚   if exc_value.errno in (errno.EACCES, errno.EAGAIN):     
    401 â”‚   â”‚   â”‚   â”‚   â”‚   raise exceptions.AlreadyLocked(                     
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BlockingIOError: [Errno 11] Resource temporarily unavailable

The above exception was the direct cause of the following exception:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:144 in _load                                                        
                                                                                
     141 â”‚   â”‚   # on import and crashes in read-only systems even if local mo  
     142 â”‚   â”‚                                                                  
     143 â”‚   â”‚   try:                                                           
  â±  144 â”‚   â”‚   â”‚   portalocker.lock(                                          
     145 â”‚   â”‚   â”‚   â”‚   self._flock_file,                                      
     146 â”‚   â”‚   â”‚   â”‚   portalocker.LockFlags.EXCLUSIVE | portalocker.LockFla  
     147 â”‚   â”‚   â”‚   )                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :441 in lock                                                                  
                                                                                
    438 â”‚                                                                       
    439 â”‚   # Public API for POSIX uses the PosixLocker instance                
    440 â”‚   def lock(file: types.FileArgument, flags: LockFlags) -> None:       
  â± 441 â”‚   â”‚   _posix_locker_instance.lock(file, flags)                        
    442 â”‚                                                                       
    443 â”‚   def unlock(file: types.FileArgument) -> None:                       
    444 â”‚   â”‚   _posix_locker_instance.unlock(file)                             
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :401 in lock                                                                  
                                                                                
    398 â”‚   â”‚   â”‚   â”‚   self.locker(fd, flags)                                  
    399 â”‚   â”‚   â”‚   except OSError as exc_value:                                
    400 â”‚   â”‚   â”‚   â”‚   if exc_value.errno in (errno.EACCES, errno.EAGAIN):     
  â± 401 â”‚   â”‚   â”‚   â”‚   â”‚   raise exceptions.AlreadyLocked(                     
    402 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   exc_value,                                      
    403 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   strerror=str(exc_value),                        
    404 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   fh=file_obj,  # Pass original file_obj          
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AlreadyLocked: [Errno 11] Resource temporarily unavailable

During handling of the above exception, another exception occurred:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:9 in <module>                        
                                                                                
     6 st.set_page_config(page_title="Shopping AI Agent", page_icon="ğŸ›ï¸")       
     7 st.title("ğŸ›ï¸ Shopping AI Agent")                                         
     8 st.markdown("Powered by **LangGraph**, **Groq**, **Supabase**, and **Qd  
  â±  9                                                                          
    10 # Initialize session state for UI history                                
    11 if "messages" not in st.session_state:                                   
    12 â”‚   st.session_state.messages = []                                       
                                                                                
  /mount/src/agentic-product-search/src/agent/graph.py:3 in <module>            
                                                                                
     1 from langgraph.graph import StateGraph, END                              
     2 from src.agent.state import AgentState                                   
  â±  3 from src.agent.nodes import supervisor_node, search_node                 
     4                                                                          
     5 # Define Conditional Routing Logic                                       
     6 def route_after_supervisor(state: AgentState) -> str:                    
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:20 in <module>           
                                                                                
     17 SUPABASE_KEY = os.environ.get("SUPABASE_KEY")                           
     18 supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)            
     19                                                                         
  â±  20 qdrant = QdrantClient(path="./qdrant_data")                             
     21 embedding_model = TextEmbedding("BAAI/bge-small-en-v1.5")               
     22                                                                         
     23 # Hardcode temperature 0.0 for deterministic tool outputs               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/qdrant_clien  
  t.py:126 in __init__                                                          
                                                                                
     123 â”‚   â”‚   â”‚   â”‚   force_disable_check_same_thread=force_disable_check_s  
     124 â”‚   â”‚   â”‚   )                                                          
     125 â”‚   â”‚   elif path is not None:                                         
  â±  126 â”‚   â”‚   â”‚   self._client = QdrantLocal(                                
     127 â”‚   â”‚   â”‚   â”‚   location=path,                                         
     128 â”‚   â”‚   â”‚   â”‚   force_disable_check_same_thread=force_disable_check_s  
     129 â”‚   â”‚   â”‚   )                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:63 in __init__                                                      
                                                                                
      60 â”‚   â”‚   self.collections: dict[str, LocalCollection] = {}              
      61 â”‚   â”‚   self.aliases: dict[str, str] = {}                              
      62 â”‚   â”‚   self._flock_file: TextIOWrapper | None = None                  
  â±   63 â”‚   â”‚   self._load()                                                   
      64 â”‚   â”‚   self._closed: bool = False                                     
      65 â”‚                                                                      
      66 â”‚   @property                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:149 in _load                                                        
                                                                                
     146 â”‚   â”‚   â”‚   â”‚   portalocker.LockFlags.EXCLUSIVE | portalocker.LockFla  
     147 â”‚   â”‚   â”‚   )                                                          
     148 â”‚   â”‚   except portalocker.exceptions.LockException:                   
  â±  149 â”‚   â”‚   â”‚   raise RuntimeError(                                        
     150 â”‚   â”‚   â”‚   â”‚   f"Storage folder {self.location} is already accessed   
     151 â”‚   â”‚   â”‚   â”‚   f" If you require concurrent access, use Qdrant serve  
     152 â”‚   â”‚   â”‚   )                                                          
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RuntimeError: Storage folder ./qdrant_data is already accessed by another 
instance of Qdrant client. If you require concurrent access, use Qdrant server 
instead.
[12:45:03] ğŸ”„ Updated app!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:83 in <module>                       
                                                                                
    80 â”‚   â”‚   â”‚   "cart": [],                                                  
    81 â”‚   â”‚   â”‚   "active_search_filters": {}                                  
    82 â”‚   â”‚   }                                                                
  â± 83 â”‚   â”‚   final_state = app_graph.invoke(initial_state)                    
    84 â”‚   â”‚   ai_response = final_state["messages"][-1]                        
    85 â”‚                                                                        
    86 â”‚   with st.chat_message("assistant"):                                   
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3  
  094 in invoke                                                                 
                                                                                
    3091 â”‚   â”‚   chunks: list[dict[str, Any] | Any] = []                        
    3092 â”‚   â”‚   interrupts: list[Interrupt] = []                               
    3093 â”‚   â”‚                                                                  
  â± 3094 â”‚   â”‚   for chunk in self.stream(                                      
    3095 â”‚   â”‚   â”‚   input,                                                     
    3096 â”‚   â”‚   â”‚   config,                                                    
    3097 â”‚   â”‚   â”‚   context=context,                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2  
  669 in stream                                                                 
                                                                                
    2666 â”‚   â”‚   â”‚   â”‚   while loop.tick():                                     
    2667 â”‚   â”‚   â”‚   â”‚   â”‚   for task in loop.match_cached_writes():            
    2668 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.output_writes(task.id, task.writes, cach  
  â± 2669 â”‚   â”‚   â”‚   â”‚   â”‚   for _ in runner.tick(                              
    2670 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [t for t in loop.tasks.values() if not t.writ  
    2671 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                     
    2672 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   get_waiter=get_waiter,                         
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_runner.p  
  y:167 in tick                                                                 
                                                                                
    164 â”‚   â”‚   elif len(tasks) == 1 and timeout is None and get_waiter is Non  
    165 â”‚   â”‚   â”‚   t = tasks[0]                                                
    166 â”‚   â”‚   â”‚   try:                                                        
  â± 167 â”‚   â”‚   â”‚   â”‚   run_with_retry(                                         
    168 â”‚   â”‚   â”‚   â”‚   â”‚   t,                                                  
    169 â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy,                                       
    170 â”‚   â”‚   â”‚   â”‚   â”‚   configurable={                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py  
  :71 in run_with_retry                                                         
                                                                                
     68 â”‚   â”‚   â”‚   # clear any writes from previous attempts                   
     69 â”‚   â”‚   â”‚   task.writes.clear()                                         
     70 â”‚   â”‚   â”‚   # run the task                                              
  â±  71 â”‚   â”‚   â”‚   return task.proc.invoke(task.input, config)                 
     72 â”‚   â”‚   except ParentCommand as exc:                                    
     73 â”‚   â”‚   â”‚   ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]            
     74 â”‚   â”‚   â”‚   cmd = exc.args[0]                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:656 in invoke                                                          
                                                                                
    653 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run = None                                      
    654 â”‚   â”‚   â”‚   â”‚   â”‚   # run in context                                    
    655 â”‚   â”‚   â”‚   â”‚   â”‚   with set_config_context(config, run) as context:    
  â± 656 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config  
    657 â”‚   â”‚   â”‚   â”‚   else:                                                   
    658 â”‚   â”‚   â”‚   â”‚   â”‚   input = step.invoke(input, config)                  
    659 â”‚   â”‚   # finish the root run                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:400 in invoke                                                          
                                                                                
    397 â”‚   â”‚   â”‚   else:                                                       
    398 â”‚   â”‚   â”‚   â”‚   run_manager.on_chain_end(ret)                           
    399 â”‚   â”‚   else:                                                           
  â± 400 â”‚   â”‚   â”‚   ret = self.func(*args, **kwargs)                            
    401 â”‚   â”‚   if self.recurse and isinstance(ret, Runnable):                  
    402 â”‚   â”‚   â”‚   return ret.invoke(input, config)                            
    403 â”‚   â”‚   return ret                                                      
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:51 in supervisor_node    
                                                                                
     48 â”‚                                                                       
     49 â”‚   structured_llm = llm.with_structured_output(RouterOutput)           
     50 â”‚   # The LLM evaluates the entire conversation history                 
  â±  51 â”‚   response: RouterOutput = structured_llm.invoke([SystemMessage(cont  
     52 â”‚                                                                       
     53 â”‚   if response.action == "search" and response.search_params:          
     54 â”‚   â”‚   # Update search filters in LangGraph State                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:3155 in invoke                                                         
                                                                                
    3152 â”‚   â”‚   â”‚   â”‚   )                                                      
    3153 â”‚   â”‚   â”‚   â”‚   with set_config_context(config) as context:            
    3154 â”‚   â”‚   â”‚   â”‚   â”‚   if i == 0:                                         
  â± 3155 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3156 â”‚   â”‚   â”‚   â”‚   â”‚   else:                                              
    3157 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3158 â”‚   â”‚   # finish the root run                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:5695 in invoke                                                         
                                                                                
    5692 â”‚   â”‚   config: RunnableConfig | None = None,                          
    5693 â”‚   â”‚   **kwargs: Any | None,                                          
    5694 â”‚   ) -> Output:                                                       
  â± 5695 â”‚   â”‚   return self.bound.invoke(                                      
    5696 â”‚   â”‚   â”‚   input,                                                     
    5697 â”‚   â”‚   â”‚   self._merge_configs(config),                               
    5698 â”‚   â”‚   â”‚   **{**self.kwargs, **kwargs},                               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:402 in invoke                                             
                                                                                
     399 â”‚   â”‚   â”‚   "AIMessage",                                               
     400 â”‚   â”‚   â”‚   cast(                                                      
     401 â”‚   â”‚   â”‚   â”‚   "ChatGeneration",                                      
  â±  402 â”‚   â”‚   â”‚   â”‚   self.generate_prompt(                                  
     403 â”‚   â”‚   â”‚   â”‚   â”‚   [self._convert_input(input)],                      
     404 â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                         
     405 â”‚   â”‚   â”‚   â”‚   â”‚   callbacks=config.get("callbacks"),                 
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1123 in generate_prompt                                   
                                                                                
    1120 â”‚   â”‚   **kwargs: Any,                                                 
    1121 â”‚   ) -> LLMResult:                                                    
    1122 â”‚   â”‚   prompt_messages = [p.to_messages() for p in prompts]           
  â± 1123 â”‚   â”‚   return self.generate(prompt_messages, stop=stop, callbacks=ca  
    1124 â”‚                                                                      
    1125 â”‚   @override                                                          
    1126 â”‚   async def agenerate_prompt(                                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:933 in generate                                           
                                                                                
     930 â”‚   â”‚   for i, m in enumerate(input_messages):                         
     931 â”‚   â”‚   â”‚   try:                                                       
     932 â”‚   â”‚   â”‚   â”‚   results.append(                                        
  â±  933 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                         
     934 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                             
     935 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                     
     936 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers e  
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1235 in _generate_with_cache                              
                                                                                
    1232 â”‚   â”‚   â”‚   â”‚   chunks.append(chunk)                                   
    1233 â”‚   â”‚   â”‚   result = generate_from_stream(iter(chunks))                
    1234 â”‚   â”‚   elif inspect.signature(self._generate).parameters.get("run_ma  
  â± 1235 â”‚   â”‚   â”‚   result = self._generate(                                   
    1236 â”‚   â”‚   â”‚   â”‚   messages, stop=stop, run_manager=run_manager, **kwarg  
    1237 â”‚   â”‚   â”‚   )                                                          
    1238 â”‚   â”‚   else:                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_groq/chat_models  
  .py:621 in _generate                                                          
                                                                                
     618 â”‚   â”‚   â”‚   **params,                                                  
     619 â”‚   â”‚   â”‚   **kwargs,                                                  
     620 â”‚   â”‚   }                                                              
  â±  621 â”‚   â”‚   response = self.client.create(messages=message_dicts, **param  
     622 â”‚   â”‚   return self._create_chat_result(response, params)              
     623 â”‚                                                                      
     624 â”‚   async def _agenerate(                                              
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/resources/chat/comple  
  tions.py:461 in create                                                        
                                                                                
     458 â”‚   â”‚                                                                  
     459 â”‚   â”‚     timeout: Override the client-level default timeout for this  
     460 â”‚   â”‚   """                                                            
  â±  461 â”‚   â”‚   return self._post(                                             
     462 â”‚   â”‚   â”‚   "/openai/v1/chat/completions",                             
     463 â”‚   â”‚   â”‚   body=maybe_transform(                                      
     464 â”‚   â”‚   â”‚   â”‚   {                                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1242   
  in post                                                                       
                                                                                
    1239 â”‚   â”‚   opts = FinalRequestOptions.construct(                          
    1240 â”‚   â”‚   â”‚   method="post", url=path, json_data=body, files=to_httpx_f  
    1241 â”‚   â”‚   )                                                              
  â± 1242 â”‚   â”‚   return cast(ResponseT, self.request(cast_to, opts, stream=str  
    1243 â”‚                                                                      
    1244 â”‚   def patch(                                                         
    1245 â”‚   â”‚   self,                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1044   
  in request                                                                    
                                                                                
    1041 â”‚   â”‚   â”‚   â”‚   â”‚   err.response.read()                                
    1042 â”‚   â”‚   â”‚   â”‚                                                          
    1043 â”‚   â”‚   â”‚   â”‚   log.debug("Re-raising status error")                   
  â± 1044 â”‚   â”‚   â”‚   â”‚   raise self._make_status_error_from_response(err.respo  
    1045 â”‚   â”‚   â”‚                                                              
    1046 â”‚   â”‚   â”‚   break                                                      
    1047                                                                        
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a 
function. Please adjust your prompt. See 'failed_generation' for more details.",
'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation':
'<function=RouterOutput>{"action": "search", "response": "Searching for men\'s 
hats", "search_params": {"query": "men\\\'s hat", "max_price_inr": null}} 
</function>'}}
[NOTE] During task with name 'supervisor' and id 
'8f15945b-614c-1e46-b96a-45845e1c9e07'
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:83 in <module>                       
                                                                                
    80 â”‚   â”‚   â”‚   "cart": [],                                                  
    81 â”‚   â”‚   â”‚   "active_search_filters": {}                                  
    82 â”‚   â”‚   }                                                                
  â± 83 â”‚   â”‚   final_state = app_graph.invoke(initial_state)                    
    84 â”‚   â”‚   ai_response = final_state["messages"][-1]                        
    85 â”‚                                                                        
    86 â”‚   with st.chat_message("assistant"):                                   
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3  
  094 in invoke                                                                 
                                                                                
    3091 â”‚   â”‚   chunks: list[dict[str, Any] | Any] = []                        
    3092 â”‚   â”‚   interrupts: list[Interrupt] = []                               
    3093 â”‚   â”‚                                                                  
  â± 3094 â”‚   â”‚   for chunk in self.stream(                                      
    3095 â”‚   â”‚   â”‚   input,                                                     
    3096 â”‚   â”‚   â”‚   config,                                                    
    3097 â”‚   â”‚   â”‚   context=context,                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2  
  669 in stream                                                                 
                                                                                
    2666 â”‚   â”‚   â”‚   â”‚   while loop.tick():                                     
    2667 â”‚   â”‚   â”‚   â”‚   â”‚   for task in loop.match_cached_writes():            
    2668 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.output_writes(task.id, task.writes, cach  
  â± 2669 â”‚   â”‚   â”‚   â”‚   â”‚   for _ in runner.tick(                              
    2670 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [t for t in loop.tasks.values() if not t.writ  
    2671 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                     
    2672 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   get_waiter=get_waiter,                         
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_runner.p  
  y:167 in tick                                                                 
                                                                                
    164 â”‚   â”‚   elif len(tasks) == 1 and timeout is None and get_waiter is Non  
    165 â”‚   â”‚   â”‚   t = tasks[0]                                                
    166 â”‚   â”‚   â”‚   try:                                                        
  â± 167 â”‚   â”‚   â”‚   â”‚   run_with_retry(                                         
    168 â”‚   â”‚   â”‚   â”‚   â”‚   t,                                                  
    169 â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy,                                       
    170 â”‚   â”‚   â”‚   â”‚   â”‚   configurable={                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py  
  :71 in run_with_retry                                                         
                                                                                
     68 â”‚   â”‚   â”‚   # clear any writes from previous attempts                   
     69 â”‚   â”‚   â”‚   task.writes.clear()                                         
     70 â”‚   â”‚   â”‚   # run the task                                              
  â±  71 â”‚   â”‚   â”‚   return task.proc.invoke(task.input, config)                 
     72 â”‚   â”‚   except ParentCommand as exc:                                    
     73 â”‚   â”‚   â”‚   ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]            
     74 â”‚   â”‚   â”‚   cmd = exc.args[0]                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:656 in invoke                                                          
                                                                                
    653 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run = None                                      
    654 â”‚   â”‚   â”‚   â”‚   â”‚   # run in context                                    
    655 â”‚   â”‚   â”‚   â”‚   â”‚   with set_config_context(config, run) as context:    
  â± 656 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config  
    657 â”‚   â”‚   â”‚   â”‚   else:                                                   
    658 â”‚   â”‚   â”‚   â”‚   â”‚   input = step.invoke(input, config)                  
    659 â”‚   â”‚   # finish the root run                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:400 in invoke                                                          
                                                                                
    397 â”‚   â”‚   â”‚   else:                                                       
    398 â”‚   â”‚   â”‚   â”‚   run_manager.on_chain_end(ret)                           
    399 â”‚   â”‚   else:                                                           
  â± 400 â”‚   â”‚   â”‚   ret = self.func(*args, **kwargs)                            
    401 â”‚   â”‚   if self.recurse and isinstance(ret, Runnable):                  
    402 â”‚   â”‚   â”‚   return ret.invoke(input, config)                            
    403 â”‚   â”‚   return ret                                                      
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:51 in supervisor_node    
                                                                                
     48 â”‚                                                                       
     49 â”‚   structured_llm = llm.with_structured_output(RouterOutput)           
     50 â”‚   # The LLM evaluates the entire conversation history                 
  â±  51 â”‚   response: RouterOutput = structured_llm.invoke([SystemMessage(cont  
     52 â”‚                                                                       
     53 â”‚   if response.action == "search" and response.search_params:          
     54 â”‚   â”‚   # Update search filters in LangGraph State                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:3155 in invoke                                                         
                                                                                
    3152 â”‚   â”‚   â”‚   â”‚   )                                                      
    3153 â”‚   â”‚   â”‚   â”‚   with set_config_context(config) as context:            
    3154 â”‚   â”‚   â”‚   â”‚   â”‚   if i == 0:                                         
  â± 3155 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3156 â”‚   â”‚   â”‚   â”‚   â”‚   else:                                              
    3157 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3158 â”‚   â”‚   # finish the root run                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:5695 in invoke                                                         
                                                                                
    5692 â”‚   â”‚   config: RunnableConfig | None = None,                          
    5693 â”‚   â”‚   **kwargs: Any | None,                                          
    5694 â”‚   ) -> Output:                                                       
  â± 5695 â”‚   â”‚   return self.bound.invoke(                                      
    5696 â”‚   â”‚   â”‚   input,                                                     
    5697 â”‚   â”‚   â”‚   self._merge_configs(config),                               
    5698 â”‚   â”‚   â”‚   **{**self.kwargs, **kwargs},                               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:402 in invoke                                             
                                                                                
     399 â”‚   â”‚   â”‚   "AIMessage",                                               
     400 â”‚   â”‚   â”‚   cast(                                                      
     401 â”‚   â”‚   â”‚   â”‚   "ChatGeneration",                                      
  â±  402 â”‚   â”‚   â”‚   â”‚   self.generate_prompt(                                  
     403 â”‚   â”‚   â”‚   â”‚   â”‚   [self._convert_input(input)],                      
     404 â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                         
     405 â”‚   â”‚   â”‚   â”‚   â”‚   callbacks=config.get("callbacks"),                 
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1123 in generate_prompt                                   
                                                                                
    1120 â”‚   â”‚   **kwargs: Any,                                                 
    1121 â”‚   ) -> LLMResult:                                                    
    1122 â”‚   â”‚   prompt_messages = [p.to_messages() for p in prompts]           
  â± 1123 â”‚   â”‚   return self.generate(prompt_messages, stop=stop, callbacks=ca  
    1124 â”‚                                                                      
    1125 â”‚   @override                                                          
    1126 â”‚   async def agenerate_prompt(                                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:933 in generate                                           
                                                                                
     930 â”‚   â”‚   for i, m in enumerate(input_messages):                         
     931 â”‚   â”‚   â”‚   try:                                                       
     932 â”‚   â”‚   â”‚   â”‚   results.append(                                        
  â±  933 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                         
     934 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                             
     935 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                     
     936 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers e  
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1235 in _generate_with_cache                              
                                                                                
    1232 â”‚   â”‚   â”‚   â”‚   chunks.append(chunk)                                   
    1233 â”‚   â”‚   â”‚   result = generate_from_stream(iter(chunks))                
    1234 â”‚   â”‚   elif inspect.signature(self._generate).parameters.get("run_ma  
  â± 1235 â”‚   â”‚   â”‚   result = self._generate(                                   
    1236 â”‚   â”‚   â”‚   â”‚   messages, stop=stop, run_manager=run_manager, **kwarg  
    1237 â”‚   â”‚   â”‚   )                                                          
    1238 â”‚   â”‚   else:                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_groq/chat_models  
  .py:621 in _generate                                                          
                                                                                
     618 â”‚   â”‚   â”‚   **params,                                                  
     619 â”‚   â”‚   â”‚   **kwargs,                                                  
     620 â”‚   â”‚   }                                                              
  â±  621 â”‚   â”‚   response = self.client.create(messages=message_dicts, **param  
     622 â”‚   â”‚   return self._create_chat_result(response, params)              
     623 â”‚                                                                      
     624 â”‚   async def _agenerate(                                              
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/resources/chat/comple  
  tions.py:461 in create                                                        
                                                                                
     458 â”‚   â”‚                                                                  
     459 â”‚   â”‚     timeout: Override the client-level default timeout for this  
     460 â”‚   â”‚   """                                                            
  â±  461 â”‚   â”‚   return self._post(                                             
     462 â”‚   â”‚   â”‚   "/openai/v1/chat/completions",                             
     463 â”‚   â”‚   â”‚   body=maybe_transform(                                      
     464 â”‚   â”‚   â”‚   â”‚   {                                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1242   
  in post                                                                       
                                                                                
    1239 â”‚   â”‚   opts = FinalRequestOptions.construct(                          
    1240 â”‚   â”‚   â”‚   method="post", url=path, json_data=body, files=to_httpx_f  
    1241 â”‚   â”‚   )                                                              
  â± 1242 â”‚   â”‚   return cast(ResponseT, self.request(cast_to, opts, stream=str  
    1243 â”‚                                                                      
    1244 â”‚   def patch(                                                         
    1245 â”‚   â”‚   self,                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1044   
  in request                                                                    
                                                                                
    1041 â”‚   â”‚   â”‚   â”‚   â”‚   err.response.read()                                
    1042 â”‚   â”‚   â”‚   â”‚                                                          
    1043 â”‚   â”‚   â”‚   â”‚   log.debug("Re-raising status error")                   
  â± 1044 â”‚   â”‚   â”‚   â”‚   raise self._make_status_error_from_response(err.respo  
    1045 â”‚   â”‚   â”‚                                                              
    1046 â”‚   â”‚   â”‚   break                                                      
    1047                                                                        
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a 
function. Please adjust your prompt. See 'failed_generation' for more details.",
'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation':
'<function=RouterOutput>{"action": "search", "response": "Searching for men\'s 
hats", "search_params": {"query": "men\\\'s hat", "max_price_inr": 
null}}</function>'}}
[NOTE] During task with name 'supervisor' and id 
'c7fd4c06-5bfa-0ae3-fefc-8592d110b748'
[12:50:32] ğŸ™ Pulling code changes from Github...
[12:50:32] ğŸ“¦ Processing dependencies...
[12:50:32] ğŸ“¦ Processed dependencies!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :398 in lock                                                                  
                                                                                
    395 â”‚   â”‚   â”‚                                                               
    396 â”‚   â”‚   â”‚   fd = self._get_fd(file_obj)                                 
    397 â”‚   â”‚   â”‚   try:                                                        
  â± 398 â”‚   â”‚   â”‚   â”‚   self.locker(fd, flags)                                  
    399 â”‚   â”‚   â”‚   except OSError as exc_value:                                
    400 â”‚   â”‚   â”‚   â”‚   if exc_value.errno in (errno.EACCES, errno.EAGAIN):     
    401 â”‚   â”‚   â”‚   â”‚   â”‚   raise exceptions.AlreadyLocked(                     
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BlockingIOError: [Errno 11] Resource temporarily unavailable

The above exception was the direct cause of the following exception:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:144 in _load                                                        
                                                                                
     141 â”‚   â”‚   # on import and crashes in read-only systems even if local mo  
     142 â”‚   â”‚                                                                  
     143 â”‚   â”‚   try:                                                           
  â±  144 â”‚   â”‚   â”‚   portalocker.lock(                                          
     145 â”‚   â”‚   â”‚   â”‚   self._flock_file,                                      
     146 â”‚   â”‚   â”‚   â”‚   portalocker.LockFlags.EXCLUSIVE | portalocker.LockFla  
     147 â”‚   â”‚   â”‚   )                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :441 in lock                                                                  
                                                                                
    438 â”‚                                                                       
    439 â”‚   # Public API for POSIX uses the PosixLocker instance                
    440 â”‚   def lock(file: types.FileArgument, flags: LockFlags) -> None:       
  â± 441 â”‚   â”‚   _posix_locker_instance.lock(file, flags)                        
    442 â”‚                                                                       
    443 â”‚   def unlock(file: types.FileArgument) -> None:                       
    444 â”‚   â”‚   _posix_locker_instance.unlock(file)                             
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :401 in lock                                                                  
                                                                                
    398 â”‚   â”‚   â”‚   â”‚   self.locker(fd, flags)                                  
    399 â”‚   â”‚   â”‚   except OSError as exc_value:                                
    400 â”‚   â”‚   â”‚   â”‚   if exc_value.errno in (errno.EACCES, errno.EAGAIN):     
  â± 401 â”‚   â”‚   â”‚   â”‚   â”‚   raise exceptions.AlreadyLocked(                     
    402 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   exc_value,                                      
    403 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   strerror=str(exc_value),                        
    404 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   fh=file_obj,  # Pass original file_obj          
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AlreadyLocked: [Errno 11] Resource temporarily unavailable

During handling of the above exception, another exception occurred:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:9 in <module>                        
                                                                                
     6 from fastembed import TextEmbedding                                      
     7                                                                          
     8 # Import our backend logic                                               
  â±  9 from src.agent.graph import app_graph                                    
    10 from src.agent.state import AgentState                                   
    11 from src.agent.nodes import qdrant  # Import the global qdrant client f  
    12                                                                          
                                                                                
  /mount/src/agentic-product-search/src/agent/graph.py:3 in <module>            
                                                                                
     1 from langgraph.graph import StateGraph, END                              
     2 from src.agent.state import AgentState                                   
  â±  3 from src.agent.nodes import supervisor_node, search_node                 
     4                                                                          
     5 # Define Conditional Routing Logic                                       
     6 def route_after_supervisor(state: AgentState) -> str:                    
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:20 in <module>           
                                                                                
     17 SUPABASE_KEY = os.environ.get("SUPABASE_KEY")                           
     18 supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)            
     19                                                                         
  â±  20 qdrant = QdrantClient(path="./qdrant_data")                             
     21 embedding_model = TextEmbedding("BAAI/bge-small-en-v1.5")               
     22                                                                         
     23 # Hardcode temperature 0.0 for deterministic tool outputs               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/qdrant_clien  
  t.py:126 in __init__                                                          
                                                                                
     123 â”‚   â”‚   â”‚   â”‚   force_disable_check_same_thread=force_disable_check_s  
     124 â”‚   â”‚   â”‚   )                                                          
     125 â”‚   â”‚   elif path is not None:                                         
  â±  126 â”‚   â”‚   â”‚   self._client = QdrantLocal(                                
     127 â”‚   â”‚   â”‚   â”‚   location=path,                                         
     128 â”‚   â”‚   â”‚   â”‚   force_disable_check_same_thread=force_disable_check_s  
     129 â”‚   â”‚   â”‚   )                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:63 in __init__                                                      
                                                                                
      60 â”‚   â”‚   self.collections: dict[str, LocalCollection] = {}              
      61 â”‚   â”‚   self.aliases: dict[str, str] = {}                              
      62 â”‚   â”‚   self._flock_file: TextIOWrapper | None = None                  
  â±   63 â”‚   â”‚   self._load()                                                   
      64 â”‚   â”‚   self._closed: bool = False                                     
      65 â”‚                                                                      
      66 â”‚   @property                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:149 in _load                                                        
                                                                                
     146 â”‚   â”‚   â”‚   â”‚   portalocker.LockFlags.EXCLUSIVE | portalocker.LockFla  
     147 â”‚   â”‚   â”‚   )                                                          
     148 â”‚   â”‚   except portalocker.exceptions.LockException:                   
  â±  149 â”‚   â”‚   â”‚   raise RuntimeError(                                        
     150 â”‚   â”‚   â”‚   â”‚   f"Storage folder {self.location} is already accessed   
     151 â”‚   â”‚   â”‚   â”‚   f" If you require concurrent access, use Qdrant serve  
     152 â”‚   â”‚   â”‚   )                                                          
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RuntimeError: Storage folder ./qdrant_data is already accessed by another 
instance of Qdrant client. If you require concurrent access, use Qdrant server 
instead.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :398 in lock                                                                  
                                                                                
    395 â”‚   â”‚   â”‚                                                               
    396 â”‚   â”‚   â”‚   fd = self._get_fd(file_obj)                                 
    397 â”‚   â”‚   â”‚   try:                                                        
  â± 398 â”‚   â”‚   â”‚   â”‚   self.locker(fd, flags)                                  
    399 â”‚   â”‚   â”‚   except OSError as exc_value:                                
    400 â”‚   â”‚   â”‚   â”‚   if exc_value.errno in (errno.EACCES, errno.EAGAIN):     
    401 â”‚   â”‚   â”‚   â”‚   â”‚   raise exceptions.AlreadyLocked(                     
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BlockingIOError: [Errno 11] Resource temporarily unavailable

The above exception was the direct cause of the following exception:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:144 in _load                                                        
                                                                                
     141 â”‚   â”‚   # on import and crashes in read-only systems even if local mo  
     142 â”‚   â”‚                                                                  
     143 â”‚   â”‚   try:                                                           
  â±  144 â”‚   â”‚   â”‚   portalocker.lock(                                          
     145 â”‚   â”‚   â”‚   â”‚   self._flock_file,                                      
     146 â”‚   â”‚   â”‚   â”‚   portalocker.LockFlags.EXCLUSIVE | portalocker.LockFla  
     147 â”‚   â”‚   â”‚   )                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :441 in lock                                                                  
                                                                                
    438 â”‚                                                                       
    439 â”‚   # Public API for POSIX uses the PosixLocker instance                
    440 â”‚   def lock(file: types.FileArgument, flags: LockFlags) -> None:       
  â± 441 â”‚   â”‚   _posix_locker_instance.lock(file, flags)                        
    442 â”‚                                                                       
    443 â”‚   def unlock(file: types.FileArgument) -> None:                       
    444 â”‚   â”‚   _posix_locker_instance.unlock(file)                             
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/portalocker/portalocker.py  
  :401 in lock                                                                  
                                                                                
    398 â”‚   â”‚   â”‚   â”‚   self.locker(fd, flags)                                  
    399 â”‚   â”‚   â”‚   except OSError as exc_value:                                
    400 â”‚   â”‚   â”‚   â”‚   if exc_value.errno in (errno.EACCES, errno.EAGAIN):     
  â± 401 â”‚   â”‚   â”‚   â”‚   â”‚   raise exceptions.AlreadyLocked(                     
    402 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   exc_value,                                      
    403 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   strerror=str(exc_value),                        
    404 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   fh=file_obj,  # Pass original file_obj          
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AlreadyLocked: [Errno 11] Resource temporarily unavailable

During handling of the above exception, another exception occurred:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:9 in <module>                        
                                                                                
     6 from fastembed import TextEmbedding                                      
     7                                                                          
     8 # Import our backend logic                                               
  â±  9 from src.agent.graph import app_graph                                    
    10 from src.agent.state import AgentState                                   
    11 from src.agent.nodes import qdrant  # Import the global qdrant client f  
    12                                                                          
                                                                                
  /mount/src/agentic-product-search/src/agent/graph.py:3 in <module>            
                                                                                
     1 from langgraph.graph import StateGraph, END                              
     2 from src.agent.state import AgentState                                   
  â±  3 from src.agent.nodes import supervisor_node, search_node                 
     4                                                                          
     5 # Define Conditional Routing Logic                                       
     6 def route_after_supervisor(state: AgentState) -> str:                    
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:20 in <module>           
                                                                                
     17 SUPABASE_KEY = os.environ.get("SUPABASE_KEY")                           
     18 supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)            
     19                                                                         
  â±  20 qdrant = QdrantClient(path="./qdrant_data")                             
     21 embedding_model = TextEmbedding("BAAI/bge-small-en-v1.5")               
     22                                                                         
     23 # Hardcode temperature 0.0 for deterministic tool outputs               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/qdrant_clien  
  t.py:126 in __init__                                                          
                                                                                
     123 â”‚   â”‚   â”‚   â”‚   force_disable_check_same_thread=force_disable_check_s  
     124 â”‚   â”‚   â”‚   )                                                          
     125 â”‚   â”‚   elif path is not None:                                         
  â±  126 â”‚   â”‚   â”‚   self._client = QdrantLocal(                                
     127 â”‚   â”‚   â”‚   â”‚   location=path,                                         
     128 â”‚   â”‚   â”‚   â”‚   force_disable_check_same_thread=force_disable_check_s  
     129 â”‚   â”‚   â”‚   )                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:63 in __init__                                                      
                                                                                
      60 â”‚   â”‚   self.collections: dict[str, LocalCollection] = {}              
      61 â”‚   â”‚   self.aliases: dict[str, str] = {}                              
      62 â”‚   â”‚   self._flock_file: TextIOWrapper | None = None                  
  â±   63 â”‚   â”‚   self._load()                                                   
      64 â”‚   â”‚   self._closed: bool = False                                     
      65 â”‚                                                                      
      66 â”‚   @property                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/qdrant_client/local/qdrant  
  _local.py:149 in _load                                                        
                                                                                
     146 â”‚   â”‚   â”‚   â”‚   portalocker.LockFlags.EXCLUSIVE | portalocker.LockFla  
     147 â”‚   â”‚   â”‚   )                                                          
     148 â”‚   â”‚   except portalocker.exceptions.LockException:                   
  â±  149 â”‚   â”‚   â”‚   raise RuntimeError(                                        
     150 â”‚   â”‚   â”‚   â”‚   f"Storage folder {self.location} is already accessed   
     151 â”‚   â”‚   â”‚   â”‚   f" If you require concurrent access, use Qdrant serve  
     152 â”‚   â”‚   â”‚   )                                                          
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RuntimeError: Storage folder ./qdrant_data is already accessed by another 
instance of Qdrant client. If you require concurrent access, use Qdrant server 
instead.
[12:50:34] ğŸ”„ Updated app!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:83 in <module>                       
                                                                                
    80 â”‚   â”‚   â”‚   "cart": [],                                                  
    81 â”‚   â”‚   â”‚   "active_search_filters": {}                                  
    82 â”‚   â”‚   }                                                                
  â± 83 â”‚   â”‚   final_state = app_graph.invoke(initial_state)                    
    84 â”‚   â”‚   ai_response = final_state["messages"][-1]                        
    85 â”‚                                                                        
    86 â”‚   with st.chat_message("assistant"):                                   
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3  
  094 in invoke                                                                 
                                                                                
    3091 â”‚   â”‚   chunks: list[dict[str, Any] | Any] = []                        
    3092 â”‚   â”‚   interrupts: list[Interrupt] = []                               
    3093 â”‚   â”‚                                                                  
  â± 3094 â”‚   â”‚   for chunk in self.stream(                                      
    3095 â”‚   â”‚   â”‚   input,                                                     
    3096 â”‚   â”‚   â”‚   config,                                                    
    3097 â”‚   â”‚   â”‚   context=context,                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2  
  669 in stream                                                                 
                                                                                
    2666 â”‚   â”‚   â”‚   â”‚   while loop.tick():                                     
    2667 â”‚   â”‚   â”‚   â”‚   â”‚   for task in loop.match_cached_writes():            
    2668 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.output_writes(task.id, task.writes, cach  
  â± 2669 â”‚   â”‚   â”‚   â”‚   â”‚   for _ in runner.tick(                              
    2670 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [t for t in loop.tasks.values() if not t.writ  
    2671 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                     
    2672 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   get_waiter=get_waiter,                         
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_runner.p  
  y:167 in tick                                                                 
                                                                                
    164 â”‚   â”‚   elif len(tasks) == 1 and timeout is None and get_waiter is Non  
    165 â”‚   â”‚   â”‚   t = tasks[0]                                                
    166 â”‚   â”‚   â”‚   try:                                                        
  â± 167 â”‚   â”‚   â”‚   â”‚   run_with_retry(                                         
    168 â”‚   â”‚   â”‚   â”‚   â”‚   t,                                                  
    169 â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy,                                       
    170 â”‚   â”‚   â”‚   â”‚   â”‚   configurable={                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py  
  :71 in run_with_retry                                                         
                                                                                
     68 â”‚   â”‚   â”‚   # clear any writes from previous attempts                   
     69 â”‚   â”‚   â”‚   task.writes.clear()                                         
     70 â”‚   â”‚   â”‚   # run the task                                              
  â±  71 â”‚   â”‚   â”‚   return task.proc.invoke(task.input, config)                 
     72 â”‚   â”‚   except ParentCommand as exc:                                    
     73 â”‚   â”‚   â”‚   ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]            
     74 â”‚   â”‚   â”‚   cmd = exc.args[0]                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:656 in invoke                                                          
                                                                                
    653 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run = None                                      
    654 â”‚   â”‚   â”‚   â”‚   â”‚   # run in context                                    
    655 â”‚   â”‚   â”‚   â”‚   â”‚   with set_config_context(config, run) as context:    
  â± 656 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config  
    657 â”‚   â”‚   â”‚   â”‚   else:                                                   
    658 â”‚   â”‚   â”‚   â”‚   â”‚   input = step.invoke(input, config)                  
    659 â”‚   â”‚   # finish the root run                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:400 in invoke                                                          
                                                                                
    397 â”‚   â”‚   â”‚   else:                                                       
    398 â”‚   â”‚   â”‚   â”‚   run_manager.on_chain_end(ret)                           
    399 â”‚   â”‚   else:                                                           
  â± 400 â”‚   â”‚   â”‚   ret = self.func(*args, **kwargs)                            
    401 â”‚   â”‚   if self.recurse and isinstance(ret, Runnable):                  
    402 â”‚   â”‚   â”‚   return ret.invoke(input, config)                            
    403 â”‚   â”‚   return ret                                                      
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:52 in supervisor_node    
                                                                                
     49 â”‚                                                                       
     50 â”‚   structured_llm = llm.with_structured_output(RouterOutput)           
     51 â”‚   # The LLM evaluates the entire conversation history                 
  â±  52 â”‚   response: RouterOutput = structured_llm.invoke([SystemMessage(cont  
     53 â”‚                                                                       
     54 â”‚   if response.action == "search" and response.search_params:          
     55 â”‚   â”‚   # Update search filters in LangGraph State                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:3155 in invoke                                                         
                                                                                
    3152 â”‚   â”‚   â”‚   â”‚   )                                                      
    3153 â”‚   â”‚   â”‚   â”‚   with set_config_context(config) as context:            
    3154 â”‚   â”‚   â”‚   â”‚   â”‚   if i == 0:                                         
  â± 3155 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3156 â”‚   â”‚   â”‚   â”‚   â”‚   else:                                              
    3157 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3158 â”‚   â”‚   # finish the root run                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:5695 in invoke                                                         
                                                                                
    5692 â”‚   â”‚   config: RunnableConfig | None = None,                          
    5693 â”‚   â”‚   **kwargs: Any | None,                                          
    5694 â”‚   ) -> Output:                                                       
  â± 5695 â”‚   â”‚   return self.bound.invoke(                                      
    5696 â”‚   â”‚   â”‚   input,                                                     
    5697 â”‚   â”‚   â”‚   self._merge_configs(config),                               
    5698 â”‚   â”‚   â”‚   **{**self.kwargs, **kwargs},                               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:402 in invoke                                             
                                                                                
     399 â”‚   â”‚   â”‚   "AIMessage",                                               
     400 â”‚   â”‚   â”‚   cast(                                                      
     401 â”‚   â”‚   â”‚   â”‚   "ChatGeneration",                                      
  â±  402 â”‚   â”‚   â”‚   â”‚   self.generate_prompt(                                  
     403 â”‚   â”‚   â”‚   â”‚   â”‚   [self._convert_input(input)],                      
     404 â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                         
     405 â”‚   â”‚   â”‚   â”‚   â”‚   callbacks=config.get("callbacks"),                 
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1123 in generate_prompt                                   
                                                                                
    1120 â”‚   â”‚   **kwargs: Any,                                                 
    1121 â”‚   ) -> LLMResult:                                                    
    1122 â”‚   â”‚   prompt_messages = [p.to_messages() for p in prompts]           
  â± 1123 â”‚   â”‚   return self.generate(prompt_messages, stop=stop, callbacks=ca  
    1124 â”‚                                                                      
    1125 â”‚   @override                                                          
    1126 â”‚   async def agenerate_prompt(                                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:933 in generate                                           
                                                                                
     930 â”‚   â”‚   for i, m in enumerate(input_messages):                         
     931 â”‚   â”‚   â”‚   try:                                                       
     932 â”‚   â”‚   â”‚   â”‚   results.append(                                        
  â±  933 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                         
     934 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                             
     935 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                     
     936 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers e  
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1235 in _generate_with_cache                              
                                                                                
    1232 â”‚   â”‚   â”‚   â”‚   chunks.append(chunk)                                   
    1233 â”‚   â”‚   â”‚   result = generate_from_stream(iter(chunks))                
    1234 â”‚   â”‚   elif inspect.signature(self._generate).parameters.get("run_ma  
  â± 1235 â”‚   â”‚   â”‚   result = self._generate(                                   
    1236 â”‚   â”‚   â”‚   â”‚   messages, stop=stop, run_manager=run_manager, **kwarg  
    1237 â”‚   â”‚   â”‚   )                                                          
    1238 â”‚   â”‚   else:                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_groq/chat_models  
  .py:621 in _generate                                                          
                                                                                
     618 â”‚   â”‚   â”‚   **params,                                                  
     619 â”‚   â”‚   â”‚   **kwargs,                                                  
     620 â”‚   â”‚   }                                                              
  â±  621 â”‚   â”‚   response = self.client.create(messages=message_dicts, **param  
     622 â”‚   â”‚   return self._create_chat_result(response, params)              
     623 â”‚                                                                      
     624 â”‚   async def _agenerate(                                              
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/resources/chat/comple  
  tions.py:461 in create                                                        
                                                                                
     458 â”‚   â”‚                                                                  
     459 â”‚   â”‚     timeout: Override the client-level default timeout for this  
     460 â”‚   â”‚   """                                                            
  â±  461 â”‚   â”‚   return self._post(                                             
     462 â”‚   â”‚   â”‚   "/openai/v1/chat/completions",                             
     463 â”‚   â”‚   â”‚   body=maybe_transform(                                      
     464 â”‚   â”‚   â”‚   â”‚   {                                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1242   
  in post                                                                       
                                                                                
    1239 â”‚   â”‚   opts = FinalRequestOptions.construct(                          
    1240 â”‚   â”‚   â”‚   method="post", url=path, json_data=body, files=to_httpx_f  
    1241 â”‚   â”‚   )                                                              
  â± 1242 â”‚   â”‚   return cast(ResponseT, self.request(cast_to, opts, stream=str  
    1243 â”‚                                                                      
    1244 â”‚   def patch(                                                         
    1245 â”‚   â”‚   self,                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1044   
  in request                                                                    
                                                                                
    1041 â”‚   â”‚   â”‚   â”‚   â”‚   err.response.read()                                
    1042 â”‚   â”‚   â”‚   â”‚                                                          
    1043 â”‚   â”‚   â”‚   â”‚   log.debug("Re-raising status error")                   
  â± 1044 â”‚   â”‚   â”‚   â”‚   raise self._make_status_error_from_response(err.respo  
    1045 â”‚   â”‚   â”‚                                                              
    1046 â”‚   â”‚   â”‚   break                                                      
    1047                                                                        
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for 
model `llama-3.3-70b-versatile` in organization `org_01kgd260zcfsr8ar1vkvceahwz`
service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98900, 
Requested 1174. Please try again in 1m3.936s. Need more tokens? Upgrade to Dev 
Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 
'code': 'rate_limit_exceeded'}}
[NOTE] During task with name 'supervisor' and id 
'2a7cdc83-cfba-3ce9-a7d6-29379484b51e'
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:83 in <module>                       
                                                                                
    80 â”‚   â”‚   â”‚   "cart": [],                                                  
    81 â”‚   â”‚   â”‚   "active_search_filters": {}                                  
    82 â”‚   â”‚   }                                                                
  â± 83 â”‚   â”‚   final_state = app_graph.invoke(initial_state)                    
    84 â”‚   â”‚   ai_response = final_state["messages"][-1]                        
    85 â”‚                                                                        
    86 â”‚   with st.chat_message("assistant"):                                   
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3  
  094 in invoke                                                                 
                                                                                
    3091 â”‚   â”‚   chunks: list[dict[str, Any] | Any] = []                        
    3092 â”‚   â”‚   interrupts: list[Interrupt] = []                               
    3093 â”‚   â”‚                                                                  
  â± 3094 â”‚   â”‚   for chunk in self.stream(                                      
    3095 â”‚   â”‚   â”‚   input,                                                     
    3096 â”‚   â”‚   â”‚   config,                                                    
    3097 â”‚   â”‚   â”‚   context=context,                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2  
  669 in stream                                                                 
                                                                                
    2666 â”‚   â”‚   â”‚   â”‚   while loop.tick():                                     
    2667 â”‚   â”‚   â”‚   â”‚   â”‚   for task in loop.match_cached_writes():            
    2668 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.output_writes(task.id, task.writes, cach  
  â± 2669 â”‚   â”‚   â”‚   â”‚   â”‚   for _ in runner.tick(                              
    2670 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [t for t in loop.tasks.values() if not t.writ  
    2671 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                     
    2672 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   get_waiter=get_waiter,                         
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_runner.p  
  y:167 in tick                                                                 
                                                                                
    164 â”‚   â”‚   elif len(tasks) == 1 and timeout is None and get_waiter is Non  
    165 â”‚   â”‚   â”‚   t = tasks[0]                                                
    166 â”‚   â”‚   â”‚   try:                                                        
  â± 167 â”‚   â”‚   â”‚   â”‚   run_with_retry(                                         
    168 â”‚   â”‚   â”‚   â”‚   â”‚   t,                                                  
    169 â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy,                                       
    170 â”‚   â”‚   â”‚   â”‚   â”‚   configurable={                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py  
  :71 in run_with_retry                                                         
                                                                                
     68 â”‚   â”‚   â”‚   # clear any writes from previous attempts                   
     69 â”‚   â”‚   â”‚   task.writes.clear()                                         
     70 â”‚   â”‚   â”‚   # run the task                                              
  â±  71 â”‚   â”‚   â”‚   return task.proc.invoke(task.input, config)                 
     72 â”‚   â”‚   except ParentCommand as exc:                                    
     73 â”‚   â”‚   â”‚   ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]            
     74 â”‚   â”‚   â”‚   cmd = exc.args[0]                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:656 in invoke                                                          
                                                                                
    653 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run = None                                      
    654 â”‚   â”‚   â”‚   â”‚   â”‚   # run in context                                    
    655 â”‚   â”‚   â”‚   â”‚   â”‚   with set_config_context(config, run) as context:    
  â± 656 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config  
    657 â”‚   â”‚   â”‚   â”‚   else:                                                   
    658 â”‚   â”‚   â”‚   â”‚   â”‚   input = step.invoke(input, config)                  
    659 â”‚   â”‚   # finish the root run                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:400 in invoke                                                          
                                                                                
    397 â”‚   â”‚   â”‚   else:                                                       
    398 â”‚   â”‚   â”‚   â”‚   run_manager.on_chain_end(ret)                           
    399 â”‚   â”‚   else:                                                           
  â± 400 â”‚   â”‚   â”‚   ret = self.func(*args, **kwargs)                            
    401 â”‚   â”‚   if self.recurse and isinstance(ret, Runnable):                  
    402 â”‚   â”‚   â”‚   return ret.invoke(input, config)                            
    403 â”‚   â”‚   return ret                                                      
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:52 in supervisor_node    
                                                                                
     49 â”‚                                                                       
     50 â”‚   structured_llm = llm.with_structured_output(RouterOutput)           
     51 â”‚   # The LLM evaluates the entire conversation history                 
  â±  52 â”‚   response: RouterOutput = structured_llm.invoke([SystemMessage(cont  
     53 â”‚                                                                       
     54 â”‚   if response.action == "search" and response.search_params:          
     55 â”‚   â”‚   # Update search filters in LangGraph State                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:3155 in invoke                                                         
                                                                                
    3152 â”‚   â”‚   â”‚   â”‚   )                                                      
    3153 â”‚   â”‚   â”‚   â”‚   with set_config_context(config) as context:            
    3154 â”‚   â”‚   â”‚   â”‚   â”‚   if i == 0:                                         
  â± 3155 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3156 â”‚   â”‚   â”‚   â”‚   â”‚   else:                                              
    3157 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3158 â”‚   â”‚   # finish the root run                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:5695 in invoke                                                         
                                                                                
    5692 â”‚   â”‚   config: RunnableConfig | None = None,                          
    5693 â”‚   â”‚   **kwargs: Any | None,                                          
    5694 â”‚   ) -> Output:                                                       
  â± 5695 â”‚   â”‚   return self.bound.invoke(                                      
    5696 â”‚   â”‚   â”‚   input,                                                     
    5697 â”‚   â”‚   â”‚   self._merge_configs(config),                               
    5698 â”‚   â”‚   â”‚   **{**self.kwargs, **kwargs},                               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:402 in invoke                                             
                                                                                
     399 â”‚   â”‚   â”‚   "AIMessage",                                               
     400 â”‚   â”‚   â”‚   cast(                                                      
     401 â”‚   â”‚   â”‚   â”‚   "ChatGeneration",                                      
  â±  402 â”‚   â”‚   â”‚   â”‚   self.generate_prompt(                                  
     403 â”‚   â”‚   â”‚   â”‚   â”‚   [self._convert_input(input)],                      
     404 â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                         
     405 â”‚   â”‚   â”‚   â”‚   â”‚   callbacks=config.get("callbacks"),                 
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1123 in generate_prompt                                   
                                                                                
    1120 â”‚   â”‚   **kwargs: Any,                                                 
    1121 â”‚   ) -> LLMResult:                                                    
    1122 â”‚   â”‚   prompt_messages = [p.to_messages() for p in prompts]           
  â± 1123 â”‚   â”‚   return self.generate(prompt_messages, stop=stop, callbacks=ca  
    1124 â”‚                                                                      
    1125 â”‚   @override                                                          
    1126 â”‚   async def agenerate_prompt(                                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:933 in generate                                           
                                                                                
     930 â”‚   â”‚   for i, m in enumerate(input_messages):                         
     931 â”‚   â”‚   â”‚   try:                                                       
     932 â”‚   â”‚   â”‚   â”‚   results.append(                                        
  â±  933 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                         
     934 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                             
     935 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                     
     936 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers e  
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1235 in _generate_with_cache                              
                                                                                
    1232 â”‚   â”‚   â”‚   â”‚   chunks.append(chunk)                                   
    1233 â”‚   â”‚   â”‚   result = generate_from_stream(iter(chunks))                
    1234 â”‚   â”‚   elif inspect.signature(self._generate).parameters.get("run_ma  
  â± 1235 â”‚   â”‚   â”‚   result = self._generate(                                   
    1236 â”‚   â”‚   â”‚   â”‚   messages, stop=stop, run_manager=run_manager, **kwarg  
    1237 â”‚   â”‚   â”‚   )                                                          
    1238 â”‚   â”‚   else:                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_groq/chat_models  
  .py:621 in _generate                                                          
                                                                                
     618 â”‚   â”‚   â”‚   **params,                                                  
     619 â”‚   â”‚   â”‚   **kwargs,                                                  
     620 â”‚   â”‚   }                                                              
  â±  621 â”‚   â”‚   response = self.client.create(messages=message_dicts, **param  
     622 â”‚   â”‚   return self._create_chat_result(response, params)              
     623 â”‚                                                                      
     624 â”‚   async def _agenerate(                                              
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/resources/chat/comple  
  tions.py:461 in create                                                        
                                                                                
     458 â”‚   â”‚                                                                  
     459 â”‚   â”‚     timeout: Override the client-level default timeout for this  
     460 â”‚   â”‚   """                                                            
  â±  461 â”‚   â”‚   return self._post(                                             
     462 â”‚   â”‚   â”‚   "/openai/v1/chat/completions",                             
     463 â”‚   â”‚   â”‚   body=maybe_transform(                                      
     464 â”‚   â”‚   â”‚   â”‚   {                                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1242   
  in post                                                                       
                                                                                
    1239 â”‚   â”‚   opts = FinalRequestOptions.construct(                          
    1240 â”‚   â”‚   â”‚   method="post", url=path, json_data=body, files=to_httpx_f  
    1241 â”‚   â”‚   )                                                              
  â± 1242 â”‚   â”‚   return cast(ResponseT, self.request(cast_to, opts, stream=str  
    1243 â”‚                                                                      
    1244 â”‚   def patch(                                                         
    1245 â”‚   â”‚   self,                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1044   
  in request                                                                    
                                                                                
    1041 â”‚   â”‚   â”‚   â”‚   â”‚   err.response.read()                                
    1042 â”‚   â”‚   â”‚   â”‚                                                          
    1043 â”‚   â”‚   â”‚   â”‚   log.debug("Re-raising status error")                   
  â± 1044 â”‚   â”‚   â”‚   â”‚   raise self._make_status_error_from_response(err.respo  
    1045 â”‚   â”‚   â”‚                                                              
    1046 â”‚   â”‚   â”‚   break                                                      
    1047                                                                        
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for 
model `llama-3.3-70b-versatile` in organization `org_01kgd260zcfsr8ar1vkvceahwz`
service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98887, 
Requested 1183. Please try again in 1m0.48s. Need more tokens? Upgrade to Dev 
Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 
'code': 'rate_limit_exceeded'}}
[NOTE] During task with name 'supervisor' and id 
'e59dbe2e-b65d-3ff5-30a9-f0d96b1a62de'
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/exec_code.py:129 in exec_func_with_error_handling                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/streamlit/runtime/scriptru  
  nner/script_runner.py:689 in code_to_exec                                     
                                                                                
  /mount/src/agentic-product-search/app.py:83 in <module>                       
                                                                                
    80 â”‚   â”‚   â”‚   "cart": [],                                                  
    81 â”‚   â”‚   â”‚   "active_search_filters": {}                                  
    82 â”‚   â”‚   }                                                                
  â± 83 â”‚   â”‚   final_state = app_graph.invoke(initial_state)                    
    84 â”‚   â”‚   ai_response = final_state["messages"][-1]                        
    85 â”‚                                                                        
    86 â”‚   with st.chat_message("assistant"):                                   
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3  
  094 in invoke                                                                 
                                                                                
    3091 â”‚   â”‚   chunks: list[dict[str, Any] | Any] = []                        
    3092 â”‚   â”‚   interrupts: list[Interrupt] = []                               
    3093 â”‚   â”‚                                                                  
  â± 3094 â”‚   â”‚   for chunk in self.stream(                                      
    3095 â”‚   â”‚   â”‚   input,                                                     
    3096 â”‚   â”‚   â”‚   config,                                                    
    3097 â”‚   â”‚   â”‚   context=context,                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2  
  669 in stream                                                                 
                                                                                
    2666 â”‚   â”‚   â”‚   â”‚   while loop.tick():                                     
    2667 â”‚   â”‚   â”‚   â”‚   â”‚   for task in loop.match_cached_writes():            
    2668 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.output_writes(task.id, task.writes, cach  
  â± 2669 â”‚   â”‚   â”‚   â”‚   â”‚   for _ in runner.tick(                              
    2670 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [t for t in loop.tasks.values() if not t.writ  
    2671 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                     
    2672 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   get_waiter=get_waiter,                         
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_runner.p  
  y:167 in tick                                                                 
                                                                                
    164 â”‚   â”‚   elif len(tasks) == 1 and timeout is None and get_waiter is Non  
    165 â”‚   â”‚   â”‚   t = tasks[0]                                                
    166 â”‚   â”‚   â”‚   try:                                                        
  â± 167 â”‚   â”‚   â”‚   â”‚   run_with_retry(                                         
    168 â”‚   â”‚   â”‚   â”‚   â”‚   t,                                                  
    169 â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy,                                       
    170 â”‚   â”‚   â”‚   â”‚   â”‚   configurable={                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py  
  :71 in run_with_retry                                                         
                                                                                
     68 â”‚   â”‚   â”‚   # clear any writes from previous attempts                   
     69 â”‚   â”‚   â”‚   task.writes.clear()                                         
     70 â”‚   â”‚   â”‚   # run the task                                              
  â±  71 â”‚   â”‚   â”‚   return task.proc.invoke(task.input, config)                 
     72 â”‚   â”‚   except ParentCommand as exc:                                    
     73 â”‚   â”‚   â”‚   ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]            
     74 â”‚   â”‚   â”‚   cmd = exc.args[0]                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:656 in invoke                                                          
                                                                                
    653 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run = None                                      
    654 â”‚   â”‚   â”‚   â”‚   â”‚   # run in context                                    
    655 â”‚   â”‚   â”‚   â”‚   â”‚   with set_config_context(config, run) as context:    
  â± 656 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config  
    657 â”‚   â”‚   â”‚   â”‚   else:                                                   
    658 â”‚   â”‚   â”‚   â”‚   â”‚   input = step.invoke(input, config)                  
    659 â”‚   â”‚   # finish the root run                                           
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langgraph/_internal/_runna  
  ble.py:400 in invoke                                                          
                                                                                
    397 â”‚   â”‚   â”‚   else:                                                       
    398 â”‚   â”‚   â”‚   â”‚   run_manager.on_chain_end(ret)                           
    399 â”‚   â”‚   else:                                                           
  â± 400 â”‚   â”‚   â”‚   ret = self.func(*args, **kwargs)                            
    401 â”‚   â”‚   if self.recurse and isinstance(ret, Runnable):                  
    402 â”‚   â”‚   â”‚   return ret.invoke(input, config)                            
    403 â”‚   â”‚   return ret                                                      
                                                                                
  /mount/src/agentic-product-search/src/agent/nodes.py:52 in supervisor_node    
                                                                                
     49 â”‚                                                                       
     50 â”‚   structured_llm = llm.with_structured_output(RouterOutput)           
     51 â”‚   # The LLM evaluates the entire conversation history                 
  â±  52 â”‚   response: RouterOutput = structured_llm.invoke([SystemMessage(cont  
     53 â”‚                                                                       
     54 â”‚   if response.action == "search" and response.search_params:          
     55 â”‚   â”‚   # Update search filters in LangGraph State                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:3155 in invoke                                                         
                                                                                
    3152 â”‚   â”‚   â”‚   â”‚   )                                                      
    3153 â”‚   â”‚   â”‚   â”‚   with set_config_context(config) as context:            
    3154 â”‚   â”‚   â”‚   â”‚   â”‚   if i == 0:                                         
  â± 3155 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3156 â”‚   â”‚   â”‚   â”‚   â”‚   else:                                              
    3157 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   input_ = context.run(step.invoke, input_, con  
    3158 â”‚   â”‚   # finish the root run                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/runnables/b  
  ase.py:5695 in invoke                                                         
                                                                                
    5692 â”‚   â”‚   config: RunnableConfig | None = None,                          
    5693 â”‚   â”‚   **kwargs: Any | None,                                          
    5694 â”‚   ) -> Output:                                                       
  â± 5695 â”‚   â”‚   return self.bound.invoke(                                      
    5696 â”‚   â”‚   â”‚   input,                                                     
    5697 â”‚   â”‚   â”‚   self._merge_configs(config),                               
    5698 â”‚   â”‚   â”‚   **{**self.kwargs, **kwargs},                               
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:402 in invoke                                             
                                                                                
     399 â”‚   â”‚   â”‚   "AIMessage",                                               
     400 â”‚   â”‚   â”‚   cast(                                                      
     401 â”‚   â”‚   â”‚   â”‚   "ChatGeneration",                                      
  â±  402 â”‚   â”‚   â”‚   â”‚   self.generate_prompt(                                  
     403 â”‚   â”‚   â”‚   â”‚   â”‚   [self._convert_input(input)],                      
     404 â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                         
     405 â”‚   â”‚   â”‚   â”‚   â”‚   callbacks=config.get("callbacks"),                 
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1123 in generate_prompt                                   
                                                                                
    1120 â”‚   â”‚   **kwargs: Any,                                                 
    1121 â”‚   ) -> LLMResult:                                                    
    1122 â”‚   â”‚   prompt_messages = [p.to_messages() for p in prompts]           
  â± 1123 â”‚   â”‚   return self.generate(prompt_messages, stop=stop, callbacks=ca  
    1124 â”‚                                                                      
    1125 â”‚   @override                                                          
    1126 â”‚   async def agenerate_prompt(                                        
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:933 in generate                                           
                                                                                
     930 â”‚   â”‚   for i, m in enumerate(input_messages):                         
     931 â”‚   â”‚   â”‚   try:                                                       
     932 â”‚   â”‚   â”‚   â”‚   results.append(                                        
  â±  933 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                         
     934 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                             
     935 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                     
     936 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers e  
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_core/language_mo  
  dels/chat_models.py:1235 in _generate_with_cache                              
                                                                                
    1232 â”‚   â”‚   â”‚   â”‚   chunks.append(chunk)                                   
    1233 â”‚   â”‚   â”‚   result = generate_from_stream(iter(chunks))                
    1234 â”‚   â”‚   elif inspect.signature(self._generate).parameters.get("run_ma  
  â± 1235 â”‚   â”‚   â”‚   result = self._generate(                                   
    1236 â”‚   â”‚   â”‚   â”‚   messages, stop=stop, run_manager=run_manager, **kwarg  
    1237 â”‚   â”‚   â”‚   )                                                          
    1238 â”‚   â”‚   else:                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/langchain_groq/chat_models  
  .py:621 in _generate                                                          
                                                                                
     618 â”‚   â”‚   â”‚   **params,                                                  
     619 â”‚   â”‚   â”‚   **kwargs,                                                  
     620 â”‚   â”‚   }                                                              
  â±  621 â”‚   â”‚   response = self.client.create(messages=message_dicts, **param  
     622 â”‚   â”‚   return self._create_chat_result(response, params)              
     623 â”‚                                                                      
     624 â”‚   async def _agenerate(                                              
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/resources/chat/comple  
  tions.py:461 in create                                                        
                                                                                
     458 â”‚   â”‚                                                                  
     459 â”‚   â”‚     timeout: Override the client-level default timeout for this  
     460 â”‚   â”‚   """                                                            
  â±  461 â”‚   â”‚   return self._post(                                             
     462 â”‚   â”‚   â”‚   "/openai/v1/chat/completions",                             
     463 â”‚   â”‚   â”‚   body=maybe_transform(                                      
     464 â”‚   â”‚   â”‚   â”‚   {                                                      
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1242   
  in post                                                                       
                                                                                
    1239 â”‚   â”‚   opts = FinalRequestOptions.construct(                          
    1240 â”‚   â”‚   â”‚   method="post", url=path, json_data=body, files=to_httpx_f  
    1241 â”‚   â”‚   )                                                              
  â± 1242 â”‚   â”‚   return cast(ResponseT, self.request(cast_to, opts, stream=str  
    1243 â”‚                                                                      
    1244 â”‚   def patch(                                                         
    1245 â”‚   â”‚   self,                                                          
                                                                                
  /home/adminuser/venv/lib/python3.13/site-packages/groq/_base_client.py:1044   
  in request                                                                    
                                                                                
    1041 â”‚   â”‚   â”‚   â”‚   â”‚   err.response.read()                                
    1042 â”‚   â”‚   â”‚   â”‚                                                          
    1043 â”‚   â”‚   â”‚   â”‚   log.debug("Re-raising status error")                   
  â± 1044 â”‚   â”‚   â”‚   â”‚   raise self._make_status_error_from_response(err.respo  
    1045 â”‚   â”‚   â”‚                                                              
    1046 â”‚   â”‚   â”‚   break                                                      
    1047                                                                        
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for 
model `llama-3.3-70b-versatile` in organization `org_01kgd260zcfsr8ar1vkvceahwz`
service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99938, 
Requested 1238. Please try again in 16m56.064s. Need more tokens? Upgrade to Dev
Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 
'code': 'rate_limit_exceeded'}}
[NOTE] During task with name 'supervisor' and id 
'3f36980d-b5d5-558d-e3ea-744cd22f5851'